# 分類木の分割方法

## 1. 分割の目的
- **目標**: データを「純度の高いグループ」に分割すること。
- 分割の条件（特徴量とその値）を選び、不純度を最小化するように分ける。

---

## 2. 分割基準の選定

### (1) ジニ不純度 (Gini Impurity)
ジニ不純度は、ノード内の「データの混在度」を測る指標。

**数式**:
$$
G(t) = 1 - \sum_{i=1}^{C} p_i^2
$$
- \( G(t) \): ノード \( t \) のジニ不純度  
- \( C \): クラスの数  
- \( p_i \): クラス \( i \) に属するサンプルの割合  

**例**:  
ノード内のクラスAとクラスBの割合が50%ずつの場合：
$$
G(t) = 1 - (0.5^2 + 0.5^2) = 0.5
$$
- 完全に純粋なノード（すべて同じクラス）の場合、ジニ不純度は \( 0 \) になる。

---

### (2) エントロピー (Entropy)
エントロピーは、データの「不確実性」を測る指標。

**数式**:
$$
H(t) = - \sum_{i=1}^{C} p_i \log_2(p_i)
$$
- \( H(t) \): ノード \( t \) のエントロピー  
- \( p_i \): クラス \( i \) に属するサンプルの割合  

**例**:  
ノード内のクラスAとクラスBの割合が50%ずつの場合：
$$
H(t) = - (0.5 \log_2(0.5) + 0.5 \log_2(0.5)) = 1
$$
- 完全に純粋なノードでは、エントロピーは \( 0 \) になる。

---

### (3) 情報利得 (Information Gain)
情報利得は、分割後にデータの「不純度」がどれだけ減少したかを測る指標。

**数式**:
$$
IG = H(parent) - \sum_{j=1}^{k} \frac{N_j}{N} H(child_j)
$$
- \( IG \): 情報利得  
- \( H(parent) \): 分割前のエントロピー  
- \( H(child_j) \): 分割後のノード \( j \) のエントロピー  
- \( N_j \): ノード \( j \) のサンプル数  
- \( N \): 親ノードのサンプル数  

---

## 3. 分割の流れ
1. **候補条件の生成**: 各特徴量とその値を基に分割条件を設定。  
   例: 「特徴量 \( X \) が 5 未満か？」
2. **不純度の計算**: ジニ不純度やエントロピーを使い、各条件の不純度を計算。
3. **最適な分割条件の選択**: 情報利得が最大になる条件を採用。

---

## 4. 数式を用いた例

### データセット
| サンプル | 特徴量 \( X \) | クラス |
|----------|----------------|--------|
| 1        | 2              | A      |
| 2        | 3              | A      |
| 3        | 10             | B      |
| 4        | 12             | B      |

### 分割条件: \( X \leq 5 \)

1. **親ノードのエントロピー**:
   - クラスA: 2サンプル、クラスB: 2サンプル  
   - エントロピー:  
     $$
     H(parent) = - (0.5 \log_2(0.5) + 0.5 \log_2(0.5)) = 1
     $$

2. **分割後のノードのエントロピー**:
   - ノード1 (\( X \leq 5 \)): クラスAが2サンプル  
     $$
     H(child_1) = - (1 \log_2(1)) = 0
     $$
   - ノード2 (\( X > 5 \)): クラスBが2サンプル  
     $$
     H(child_2) = - (1 \log_2(1)) = 0
     $$

3. **情報利得**:
   $$
   IG = H(parent) - \left( \frac{2}{4} H(child_1) + \frac{2}{4} H(child_2) \right)
   $$
   $$
   IG = 1 - (0 + 0) = 1
   $$

---

## 5. 分割基準の選択
- **ジニ不純度**: 計算が軽量でCARTアルゴリズムでよく使われる。
- **エントロピー**: 情報利得を基にした分割に適しており、ID3やC4.5アルゴリズムで使用される。

---

## 6. まとめ
- 決定木の分割は「データの不純度を最小化」するために最適な条件を選ぶプロセス。
- ジニ不純度とエントロピーのどちらを使うかはアルゴリズムに依存する。
