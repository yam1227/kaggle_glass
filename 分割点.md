# 分類木における分割点の選択とインデックスのソート

## 1. 分割基準の選択

分類木では、各ノードで最も情報を得られる特徴量に基づいてデータを分割します。主に以下の2つの基準を使用します。

### ジニ不純度 (Gini Impurity)
ジニ不純度は、データセットがどれだけ純粋か（同じクラスに属するサンプルが多いか）を測定します。ジニ不純度が小さいほど、ノードは純粋です。ジニ不純度は次のように計算されます：

\[
Gini = 1 - \sum_{i=1}^{k} p_i^2
\]

ここで、\(p_i\)はクラス \(i\) の確率です。

### 情報利得 (Information Gain)
情報利得は、分割前後のエントロピーの変化を示します。エントロピーはデータの不確実性を示し、分割後にエントロピーが低くなるほど、情報利得が大きくなります。エントロピーは次のように定義されます：

\[
Entropy = - \sum_{i=1}^{k} p_i \log_2(p_i)
\]

情報利得は、分割前のエントロピーから分割後のエントロピーを引いたものです。

## 2. 分割点の選択方法

1. 各特徴量（特徴列）のすべての可能な分割点を候補として評価します。
2. その中で、最もジニ不純度や情報利得が最大となる分割点を選びます。

## 3. インデックスのソート

分類木アルゴリズムでは、効率的な分割点の探索を行うために、特徴量のインデックスをソートすることがあります。ソートを行うことで、以下のような効率的な処理が可能になります：

### スライディングウィンドウ技法
特徴量を昇順にソートすると、分割点の評価を効率的に行うことができます。具体的には、次の手順で分割点を選びます：

1. 各特徴量に対して値を昇順にソートします。
2. 隣接する異なるクラス間で分割できる場所を評価します。
3. 最も情報利得が高い、またはジニ不純度が低い分割点を選びます。

### ソートの利点
- ソートにより、分割点を効率的に探索でき、計算量が削減されます。

## まとめ

- **分割点の選択**は、ジニ不純度や情報利得を基に最適な分割点を決定します。
- **インデックスのソート**は、効率的な分割点探索を実現するための技法であり、計算量を削減します。
